{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import checks\n",
    "from pip._internal import main as pipmain\n",
    "\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "except:\n",
    "    pipmain([\"install\", \"nltk\"])\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "try:\n",
    "    import re\n",
    "except:\n",
    "    pipmain([\"install\", \"re\"])\n",
    "    import re\n",
    "try:\n",
    "    import string\n",
    "except:\n",
    "    pipmain([\"install\", \"string\"])\n",
    "    import string\n",
    "try:\n",
    "    import unicodedata\n",
    "except:\n",
    "    pipmain([\"install\", \"unicodedata\"])\n",
    "    import unicodedata\n",
    "try:\n",
    "    import pandas\n",
    "except:\n",
    "    pipmain([\"install\", \"pandas\"])\n",
    "    import pandas\n",
    "try:\n",
    "    import os\n",
    "except:\n",
    "    pipmain([\"install\", \"os\"])\n",
    "    import os\n",
    "try:\n",
    "    import collections\n",
    "except:\n",
    "    pipmain([\"install\", \"collections\"])\n",
    "    import collections\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "except:\n",
    "    pipmain([\"install\", \"sklearn\"])\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "try:\n",
    "    import tweepy\n",
    "except:\n",
    "    pipmain([\"install\", \"tweepy\"])\n",
    "    import tweepy\n",
    "try:\n",
    "    import csv\n",
    "except:\n",
    "    pipmain([\"install\", \"csv\"])\n",
    "    import csv\n",
    "try:\n",
    "    import time\n",
    "except:\n",
    "    pipmain([\"install\", \"time\"])\n",
    "    import time\n",
    "try:\n",
    "    import pickle\n",
    "except:\n",
    "    pipmain([\"install\", \"pickle\"])\n",
    "    import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import unicodedata\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters = string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z ]', '', text)\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = stem_text(text) # stemming\n",
    "    try:\n",
    "        text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classifier, open(\"ModelB\", 'wb'))\n",
    "BritishDF.to_csv(\"BritishCleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File location found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8103104405133541\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4bff2027f2ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File location found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ModelB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pickle Model Loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4bff2027f2ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\Honours\\Master\\Models\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ModelB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mBritishDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BritishCleaned.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "    print(\"File location found\")\n",
    "    classifier = pickle.load(open(\"ModelB\", 'rb'))\n",
    "    print(\"Pickle Model Loaded\")\n",
    "    BritishDF = pandas.DataFrame.from_csv(\"BritishCleaned.csv\")\n",
    "    print(\"CSV loaded\")\n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "    print(\"Vector built\")\n",
    "    BritishDF['text'] = BritishDF['text'].values.astype('U')\n",
    "    vectorizer.fit(BritishDF['text'].values.astype('U'))\n",
    "    print(\"Data fit\")\n",
    "    sentences = BritishDF['text'].values\n",
    "    y = BritishDF['Alignment'].values\n",
    "\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "    score = classifier.score(X_test, y_test)\n",
    "except:\n",
    "    os.chdir(\"E:\\Honours\\Master\\Twitter-Learning-Data\\Collections\")\n",
    "\n",
    "    BritishDF = pandas.DataFrame.from_csv(\"BritishCollection.csv\")\n",
    "\n",
    "    BritishDF['text'] = BritishDF['text'].apply(lambda row: clean_text(row))\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "\n",
    "    vectorizer.fit(BritishDF['text'])\n",
    "\n",
    "    sentences = BritishDF['text'].values\n",
    "    y = BritishDF['Alignment'].values\n",
    "\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test  = vectorizer.transform(sentences_test)\n",
    "    X_train\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "\n",
    "    print(\"Accuracy:\", score)\n",
    "\n",
    "    os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "    pickle.dump(classifier, open(\"ModelB\", 'wb'))\n",
    "    BritishDF.to_csv(\"BritishCleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import unicodedata\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def TweetbScrub(text, ):\n",
    "    if text[0] == \"b\":\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters = string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z ]', '', text)\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = stem_text(text) # stemming\n",
    "    try:\n",
    "        text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text\n",
    "\n",
    "PartyColours = {\"Labour\": \"#dc241f\", \n",
    "              \"Conservative\": \"#0087dc\", \n",
    "              \"SNP\": \"#fff95d\", \n",
    "              \"Green\": \"#6ab023\", \n",
    "              \"UKIP\": \"#70147a\", \n",
    "              \"DUP\":\"#d46a4c\", \n",
    "              \"Non-Aligned\": \"#ffffff\", \n",
    "              \"LibDem\": \"#fdbb30\"}\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"TC98w89JxQK2v4vPEqLLxJLx0\"\n",
    "consumer_secret = \"le4t2JCgoT3CBZwToaKdOJx5LFYDDkGL5e3Pjl2ZtfTqYV46Fs\"\n",
    "access_key = \"4459846396-tU9aYf4E5r9eHhJnniU7OsyrLNJhzEd4cpVeFFe\"\n",
    "access_secret = \"UaY6kpdXbdV7cAsAxrKLzFTkKSLtW8dcNTe1CYniUl6xM\"\n",
    "\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\n",
    "\n",
    "\n",
    "\n",
    "    outtweets = []\n",
    "    for tweet in alltweets:\n",
    "        media_count = 0\n",
    "        url_count =0\n",
    "        if \"media\" in tweet.entities:\n",
    "            media_count =1\n",
    "        if \"urls\" in tweet.entities:\n",
    "            url_count =len(tweet.entities[\"urls\"])\n",
    "        outtweets.append([tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"),tweet.retweet_count,tweet.favorite_count,media_count,url_count])\n",
    "\n",
    "    #write the csv\n",
    "    with open(screen_name + '.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"retweet_count\",\"favorite_count\",\"media_count\",\"url_count\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass\n",
    "\n",
    "def GeneratePie(collection, name):\n",
    "    import plotly \n",
    "    plotly.tools.set_credentials_file(username='andriy93', api_key='j4UgdivFmCTlwkO1bYWS')\n",
    "\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "    labels = list(collection.keys())\n",
    "    values = list(collection.values())\n",
    "    #colors = [PartyColours[i] for i in labels]\n",
    "\n",
    "\n",
    "    trace = go.Pie(labels=labels, values=values,\n",
    "                   hoverinfo='label+percent', textinfo='value', \n",
    "                   textfont=dict(size=10),\n",
    "                   marker=dict(\n",
    "                               line=dict(color='#000000', width=2)))\n",
    "\n",
    "    data = [trace]\n",
    "    layout = go.Layout(title=name)\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "\n",
    "    # Save the figure as a png image:\n",
    "    py.image.save_as(fig, name + '.png')\n",
    "\n",
    "\n",
    "def AnalyseSpeech(Filename):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Speeches\")\n",
    "    Filename = Filename.replace(\".txt\", \"\")\n",
    "    Speech = open(Filename + \".txt\").read()\n",
    "    Sentence = vectorizer.transform([Speech])\n",
    "    classifier.predict(Sentence)\n",
    "    Speech = Speech.replace(\"\\n\", \"\")\n",
    "    Speech = Speech.split(\".\")\n",
    "    for sentence in Speech:\n",
    "        if sentence == \" \":\n",
    "            Speech.remove(sentence)\n",
    "    Speech = list(filter(None, Speech))\n",
    "\n",
    "    Speech = pandas.DataFrame(Speech)\n",
    "    Speech[\"Classification\"] = \"\"\n",
    "    Speech.columns = ['Text', \"Classification\"]\n",
    "    Speech['CleanText'] = Speech['Text'].apply(lambda row: clean_text(row))\n",
    "    Sentence = vectorizer.transform(Speech[\"CleanText\"])\n",
    "    Classification = classifier.predict(Sentence)\n",
    "    Speech[\"Classification\"] = Classification\n",
    "    Speech = Speech.apply(Sentimentclassifer,axis=1)\n",
    "    Speech.drop('CleanText', axis=1, inplace=True)\n",
    "    Speechcollection = collections.Counter(Speech[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    os.chdir(\"E:\\Honours\\Master\\Speeches\\Classifications\\American\")\n",
    "    #Save Classifications to classificaiton directory\n",
    "    try:\n",
    "        os.mkdir(Filename)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Filename)\n",
    "    Speech.to_csv(Filename + \"Classification.csv\", encoding='utf-8')\n",
    "\n",
    "    GeneratePie(Speechcollection, Filename)\n",
    "\n",
    "def AnalyseUser(Username):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Classifications\")\n",
    "    try:\n",
    "        os.mkdir(Username)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Username)\n",
    "    get_all_tweets(Username)\n",
    "    Username = Username.replace(\".csv\", \"\")\n",
    "    UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "    #Vectorize the text\n",
    "    \n",
    "    UserData['CleanText'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "    Sentences = vectorizer.transform(UserData['CleanText'])\n",
    "    #Perform prediction of text inputted\n",
    "    Classification = classifier.predict(Sentences)\n",
    "    #Output a basic overview of the predictions\n",
    "    #Append original data with model classification\n",
    "    UserData[\"Classification\"] = Classification\n",
    "    UserData = UserData.apply(Sentimentclassifer,axis=1)\n",
    "    UserData.drop('CleanText', axis=1, inplace=True)\n",
    "    Usercollection = collections.Counter(UserData[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    #Save Classifications to classificaiton directory\n",
    "    UserData.to_csv(Username + \"-Classification.csv\", encoding='utf-8')\n",
    "    os.remove(Username + \".csv\")\n",
    "    \n",
    "    \n",
    "    GeneratePie(Usercollection, Username)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def Sentimentclassifer(sentence, ):\n",
    "    sentiment = sid.polarity_scores(sentence['CleanText'])\n",
    "    sentence['Sentiment'] = sentiment['compound']\n",
    "    if (sentence['Sentiment'] < -0) & (sentence['Classification'] != \"Non-Aligned\"):\n",
    "        sentence['Classification'] = \"Anti-\" + sentence['Classification']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File location found\n",
      "Pickle Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded\n",
      "Vector built\n",
      "Data fit\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "print(\"File location found\")\n",
    "classifier = pickle.load(open(\"ModelB\", 'rb'))\n",
    "print(\"Pickle Model Loaded\")\n",
    "BritishDF = pandas.DataFrame.from_csv(\"BritishCleaned.csv\")\n",
    "print(\"CSV loaded\")\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "print(\"Vector built\")\n",
    "BritishDF['text'] = BritishDF['text'].values.astype('U')\n",
    "vectorizer.fit(BritishDF['text'].values.astype('U'))\n",
    "print(\"Data fit\")\n",
    "sentences = BritishDF['text'].values\n",
    "y = BritishDF['Alignment'].values\n",
    "\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "score = classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Raw\")\n",
    "\n",
    "for filename in os.listdir():\n",
    "        if \".csv\" in filename: \n",
    "            AnalyseUser(filename)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Speeches\")\n",
    "for filename in os.listdir():\n",
    "        if \".txt\" in filename: \n",
    "            AnalyseSpeech(filename)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1019652846143332351\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 938539256687091711\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 864856539148013573\n",
      "...796 tweets downloaded so far\n",
      "getting tweets before 793038586983612415\n",
      "...996 tweets downloaded so far\n",
      "getting tweets before 726359276982276095\n",
      "...1193 tweets downloaded so far\n",
      "getting tweets before 662598920963538943\n",
      "...1393 tweets downloaded so far\n",
      "getting tweets before 611465463911043071\n",
      "...1593 tweets downloaded so far\n",
      "getting tweets before 594436032134807551\n",
      "...1793 tweets downloaded so far\n",
      "getting tweets before 553250259808571392\n",
      "...1992 tweets downloaded so far\n",
      "getting tweets before 514174070083293183\n",
      "...2188 tweets downloaded so far\n",
      "getting tweets before 498446739624714240\n",
      "...2388 tweets downloaded so far\n",
      "getting tweets before 458343379584442367\n",
      "...2587 tweets downloaded so far\n",
      "getting tweets before 365329542316961792\n",
      "...2787 tweets downloaded so far\n",
      "getting tweets before 196354344193495039\n",
      "...2985 tweets downloaded so far\n",
      "getting tweets before 6827388251\n",
      "...3185 tweets downloaded so far\n",
      "getting tweets before 2032319263\n",
      "...3196 tweets downloaded so far\n",
      "getting tweets before 1983660449\n",
      "...3196 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:193: FutureWarning:\n",
      "\n",
      "from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AnalyseUser(\"wildfireone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
