{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Nick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Import checks\n",
    "from pip._internal import main as pipmain\n",
    "\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('vader_lexicon')\n",
    "except:\n",
    "\n",
    "    pipmain([\"install\", \"nltk\"])\n",
    "    nltk.download('stopwords')\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    import re\n",
    "except:\n",
    "    pipmain([\"install\", \"re\"])\n",
    "    import re\n",
    "try:\n",
    "    import string\n",
    "except:\n",
    "    pipmain([\"install\", \"string\"])\n",
    "    import string\n",
    "try:\n",
    "    import unicodedata\n",
    "except:\n",
    "    pipmain([\"install\", \"unicodedata\"])\n",
    "    import unicodedata\n",
    "try:\n",
    "    import pandas\n",
    "except:\n",
    "    pipmain([\"install\", \"pandas\"])\n",
    "    import pandas\n",
    "try:\n",
    "    import os\n",
    "except:\n",
    "    pipmain([\"install\", \"os\"])\n",
    "    import os\n",
    "try:\n",
    "    import collections\n",
    "except:\n",
    "    pipmain([\"install\", \"collections\"])\n",
    "    import collections\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "except:\n",
    "    pipmain([\"install\", \"sklearn\"])\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "try:\n",
    "    import tweepy\n",
    "except:\n",
    "    pipmain([\"install\", \"tweepy\"])\n",
    "    import tweepy\n",
    "try:\n",
    "    import csv\n",
    "except:\n",
    "    pipmain([\"install\", \"csv\"])\n",
    "    import csv\n",
    "try:\n",
    "    import time\n",
    "except:\n",
    "    pipmain([\"install\", \"time\"])\n",
    "    import time\n",
    "try:\n",
    "    import pickle\n",
    "except:\n",
    "    pipmain([\"install\", \"pickle\"])\n",
    "    import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import unicodedata\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def TweetbScrub(text, ):\n",
    "    if text[0] == \"b\":\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters = string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = TweetbScrub(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z ]', '', text)\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = stem_text(text) # stemming\n",
    "    try:\n",
    "        text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text\n",
    "\n",
    "PartyColours = {\"Labour\": \"#dc241f\", \n",
    "              \"Conservative\": \"#0087dc\", \n",
    "              \"SNP\": \"#fff95d\", \n",
    "              \"Green\": \"#6ab023\", \n",
    "              \"UKIP\": \"#70147a\", \n",
    "              \"DUP\":\"#d46a4c\", \n",
    "              \"Non-Aligned\": \"#ffffff\", \n",
    "              \"LibDem\": \"#fdbb30\"}\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"TC98w89JxQK2v4vPEqLLxJLx0\"\n",
    "consumer_secret = \"le4t2JCgoT3CBZwToaKdOJx5LFYDDkGL5e3Pjl2ZtfTqYV46Fs\"\n",
    "access_key = \"4459846396-tU9aYf4E5r9eHhJnniU7OsyrLNJhzEd4cpVeFFe\"\n",
    "access_secret = \"UaY6kpdXbdV7cAsAxrKLzFTkKSLtW8dcNTe1CYniUl6xM\"\n",
    "\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\n",
    "\n",
    "\n",
    "\n",
    "    outtweets = []\n",
    "    for tweet in alltweets:\n",
    "        media_count = 0\n",
    "        url_count =0\n",
    "        if \"media\" in tweet.entities:\n",
    "            media_count =1\n",
    "        if \"urls\" in tweet.entities:\n",
    "            url_count =len(tweet.entities[\"urls\"])\n",
    "        outtweets.append([tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"),tweet.retweet_count,tweet.favorite_count,media_count,url_count])\n",
    "\n",
    "    #write the csv\n",
    "    with open(screen_name + '.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"retweet_count\",\"favorite_count\",\"media_count\",\"url_count\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass\n",
    "\n",
    "def get_all_favourites(screen_name):\n",
    "    \n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.favorites(screen_name = screen_name,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.favorites(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\n",
    "\n",
    "\n",
    "\n",
    "    outtweets = []\n",
    "    for tweet in alltweets:\n",
    "        media_count = 0\n",
    "        url_count =0\n",
    "        if \"media\" in tweet.entities:\n",
    "            media_count =1\n",
    "        if \"urls\" in tweet.entities:\n",
    "            url_count =len(tweet.entities[\"urls\"])\n",
    "        outtweets.append([tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"),tweet.retweet_count,tweet.favorite_count,media_count,url_count])\n",
    "\n",
    "    #write the csv\n",
    "    with open(screen_name + '.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"retweet_count\",\"favorite_count\",\"media_count\",\"url_count\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass\n",
    "\n",
    "#Remove Retweets\n",
    "def Smallclean(text, ):\n",
    "    text = TweetbScrub(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z@1-9 ]', '', text)\n",
    "    return text\n",
    "\n",
    "def FindTop(UserData):\n",
    "    \n",
    "    UserData = UserData[~UserData.text.str.contains(\"RT \")]\n",
    "    favourite = UserData.loc[UserData['favorite_count'].idxmax()]\n",
    "    retweet = UserData.loc[UserData['retweet_count'].idxmax()]\n",
    "    file = open(\"Favourite.txt\",\"w\") \n",
    "    file.writelines(favourite['text'] + \"\\n\")\n",
    "    file.writelines(str(favourite['favorite_count']))\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"Retweet.txt\",\"w\") \n",
    "    file.writelines(retweet['text'] + \"\\n\")\n",
    "    file.writelines(str(retweet['retweet_count']))\n",
    "    file.close()\n",
    "\n",
    "def GeneratePie(collection, name):\n",
    "    import plotly \n",
    "    plotly.tools.set_credentials_file(username='itjallingii', api_key='hiFqLEhfwSwdDlsj9lT8')\n",
    "\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "    labels = list(collection.keys())\n",
    "    values = list(collection.values())\n",
    "    colors = [PartyColours[i] for i in labels]\n",
    "\n",
    "\n",
    "    trace = go.Pie(labels=labels, values=values,\n",
    "                   hoverinfo='label+percent', textinfo='value', \n",
    "                   textfont=dict(size=10),\n",
    "                   marker=dict(colors = colors,\n",
    "                               line=dict(color='#000000', width=2)))\n",
    "\n",
    "    data = [trace]\n",
    "    layout = go.Layout(title=name)\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "\n",
    "    # Save the figure as a png image:\n",
    "    py.image.save_as(fig, name + '.png')\n",
    "\n",
    "def GeneratePieS(collection, name):\n",
    "    import plotly \n",
    "    plotly.tools.set_credentials_file(username='itjallingii', api_key='hiFqLEhfwSwdDlsj9lT8')\n",
    "\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "    labels = list(collection.keys())\n",
    "    values = list(collection.values())\n",
    "    #colors = [PartyColours[i] for i in labels]\n",
    "\n",
    "\n",
    "    trace = go.Pie(labels=labels, values=values,\n",
    "                   hoverinfo='label+percent', textinfo='value', \n",
    "                   textfont=dict(size=10),\n",
    "                   marker=dict(\n",
    "                               line=dict(color='#000000', width=2)))\n",
    "\n",
    "    data = [trace]\n",
    "    layout = go.Layout(title=name)\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "\n",
    "    # Save the figure as a png image:\n",
    "    py.image.save_as(fig, name + '.png')\n",
    "\n",
    "def AnalyseSpeech(Filename, Mode):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Speeches\")\n",
    "    Filename = Filename.replace(\".txt\", \"\")\n",
    "    Speech = open(Filename + \".txt\").read()\n",
    "    Sentence = vectorizer.transform([Speech])\n",
    "    classifier.predict(Sentence)\n",
    "    Speech = Speech.replace(\"\\n\", \"\")\n",
    "    Speech = Speech.split(\".\")\n",
    "    for sentence in Speech:\n",
    "        if sentence == \" \":\n",
    "            Speech.remove(sentence)\n",
    "    Speech = list(filter(None, Speech))\n",
    "\n",
    "    Speech = pandas.DataFrame(Speech)\n",
    "    Speech[\"Classification\"] = \"\"\n",
    "    Speech.columns = ['Text', \"Classification\"]\n",
    "    Speech['CleanText'] = Speech['Text'].apply(lambda row: clean_text(row))\n",
    "    Sentence = vectorizer.transform(Speech[\"CleanText\"])\n",
    "    Classification = classifier.predict(Sentence)\n",
    "    Speech[\"Classification\"] = Classification\n",
    "    SpeechS = Speech\n",
    "    if Mode == \"British\":\n",
    "        SpeechS = Speech.apply(SentimentclassiferBritish,axis=1)\n",
    "    else:\n",
    "        SpeechS = Speech.apply(SentimentclassiferAmerican,axis=1)\n",
    "    Speech.drop('CleanText', axis=1, inplace=True)\n",
    "    SpeechS.drop('CleanText', axis=1, inplace=True)\n",
    "    Speechcollection = collections.Counter(Speech[\"Classification\"])\n",
    "    SpeechScollection = collections.Counter(SpeechS[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    if Mode == \"British\":\n",
    "        os.chdir(\"E:\\Honours\\Master\\Speeches\\Classifications\\British\")\n",
    "    else:\n",
    "        os.chdir(\"E:\\Honours\\Master\\Speeches\\Classifications\\American\")\n",
    "    #Save Classifications to classificaiton directory\n",
    "    try:\n",
    "        os.mkdir(Filename)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Filename)\n",
    "    Speech.to_csv(Filename + \"-Classification.csv\", encoding='utf-8')\n",
    "    GeneratePie(Speechcollection, Filename)\n",
    "    Speech.to_csv(Filename + \"-Sentiment-Classification.csv\", encoding='utf-8')\n",
    "    if Mode == \"British\":\n",
    "        GeneratePieS(SpeechScollection, Filename + \"-Sentiment\")\n",
    "    else:\n",
    "        GeneratePie(SpeechScollection, Filename + \"-Sentiment\")\n",
    "\n",
    "def AnalyseTweets(Username, Mode):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Classifications\")\n",
    "    try:\n",
    "        os.mkdir(Username)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Username)\n",
    "\n",
    "    get_all_tweets(Username)\n",
    "    Username = Username.replace(\".csv\", \"\")\n",
    "    UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "    FindTop(UserData)\n",
    "    #Vectorize the text\n",
    "    \n",
    "    UserData['CleanText'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "    Sentences = vectorizer.transform(UserData['CleanText'])\n",
    "    #Perform prediction of text inputted\n",
    "    Classification = classifier.predict(Sentences)\n",
    "    #Output a basic overview of the predictions\n",
    "    #Append original data with model classification\n",
    "    UserData[\"Classification\"] = Classification\n",
    "    UserDataSentiment = UserData\n",
    "    \n",
    "    Usercollection = collections.Counter(UserData[\"Classification\"])\n",
    "    w = csv.writer(open(\"output.csv\", \"w\"))\n",
    "    for key, val in Usercollection.items():\n",
    "            w.writerow([key, val])\n",
    "    UserData.to_csv(Username + \"-Tweets-Classification.csv\", encoding='utf-8')\n",
    "    GeneratePie(Usercollection, Username + \"-Tweets\")\n",
    "\n",
    "    if Mode == \"British\":\n",
    "        UserDataSentiment = UserDataSentiment.apply(SentimentclassiferBritish,axis=1)\n",
    "    else:\n",
    "        UserDataSentiment = UserDataSentiment.apply(SentimentclassiferAmerican,axis=1)\n",
    "    UserData.drop('CleanText', axis=1, inplace=True)\n",
    "    UserDataSentiment.drop('CleanText', axis=1, inplace=True)\n",
    "    UsercollectionSentiment = collections.Counter(UserDataSentiment[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    #Save Classifications to classificaiton directory\n",
    "    \n",
    "    UserData.to_csv(Username + \"-Tweets-Classification.csv\", encoding='utf-8')\n",
    "    GeneratePie(Usercollection, Username + \"-Tweets\")\n",
    "    UserDataSentiment.to_csv(Username + \"-Tweets-Classification-S.csv\", encoding='utf-8')\n",
    "    if Mode == \"British\":\n",
    "        GeneratePieS(UsercollectionSentiment, Username + \"-Tweets-S\")\n",
    "    else:\n",
    "        GeneratePie(UsercollectionSentiment, Username + \"-Tweets-S\")\n",
    "\n",
    "    os.remove(Username + \".csv\")\n",
    "\n",
    "    \n",
    "def AnalyseFavourites(Username, Mode):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Classifications\")\n",
    "    try:\n",
    "        os.mkdir(Username)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Username)\n",
    "    get_all_favourites(Username)\n",
    "    Username = Username.replace(\".csv\", \"\")\n",
    "    UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "    #Vectorize the text\n",
    "    \n",
    "    UserData['CleanText'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "    Sentences = vectorizer.transform(UserData['CleanText'])\n",
    "    #Perform prediction of text inputted\n",
    "    Classification = classifier.predict(Sentences)\n",
    "    #Output a basic overview of the predictions\n",
    "    #Append original data with model classification\n",
    "    UserData[\"Classification\"] = Classification\n",
    "    UserDataSentiment = UserData\n",
    "\n",
    "\n",
    "    Usercollection = collections.Counter(UserData[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    #Save Classifications to classificaiton directory\n",
    "    \n",
    "    if Mode == \"British\":\n",
    "        UserDataSentiment = UserDataSentiment.apply(SentimentclassiferBritish,axis=1)\n",
    "    else:\n",
    "        UserDataSentiment = UserDataSentiment.apply(SentimentclassiferAmerican,axis=1)\n",
    "    \n",
    "    UserData.drop('CleanText', axis=1, inplace=True)\n",
    "    UserDataSentiment.drop('CleanText', axis=1, inplace=True)\n",
    "    UsercollectionSentiment = collections.Counter(UserDataSentiment[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    #Save Classifications to classificaiton directory\n",
    "    UserDataSentiment.to_csv(Username + \"-Favourites-Classification-S.csv\", encoding='utf-8')\n",
    "    if Mode == \"British\":\n",
    "        GeneratePieS(UsercollectionSentiment, Username + \"-Favourites-S\")\n",
    "    else:\n",
    "        GeneratePie(UsercollectionSentiment, Username + \"-Favourites-S\")\n",
    "    UserData.to_csv(Username + \"-Favourites-Classification.csv\", encoding='utf-8')\n",
    "    GeneratePie(Usercollection, Username + \"-Favourites\")\n",
    "    os.remove(Username + \".csv\")\n",
    "    \n",
    "def AnalyseUser(Username, Mode):\n",
    "    AnalyseTweets(Username, Mode)\n",
    "    AnalyseFavourites(Username, Mode)\n",
    "\n",
    "    \n",
    "    \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def SentimentclassiferBritish(sentence, ):\n",
    "    sentiment = sid.polarity_scores(sentence['CleanText'])\n",
    "    sentence['Sentiment'] = sentiment['compound']\n",
    "    if (sentence['Sentiment'] < -0) & (sentence['Classification'] != \"Non-Aligned\"):\n",
    "        sentence['Classification'] = \"Anti-\" + sentence['Classification']\n",
    "    return sentence\n",
    "def SentimentclassiferAmerican(sentence, ):\n",
    "    sentiment = sid.polarity_scores(sentence['CleanText'])\n",
    "    sentence['Sentiment'] = sentiment['compound']\n",
    "    if (sentence['Sentiment'] < -0) & (sentence['Classification'] == \"Republican\"):\n",
    "        sentence['Classification'] = \"Democratic\"\n",
    "    elif (sentence['Sentiment'] < -0) & (sentence['Classification'] == \"Democratic\"):\n",
    "        sentence['Classification'] = \"Republican\"\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load British\n",
    "def LoadBritish():\n",
    "    global Mode \n",
    "    global vectorizer\n",
    "    global classifier\n",
    "    global PartyColours \n",
    "    Mode = \"British\"\n",
    "    print(\"Loading Mode: \" + Mode) \n",
    "    PartyColours = {\"Labour\": \"#dc241f\", \n",
    "                  \"Conservative\": \"#0087dc\", \n",
    "                  \"SNP\": \"#fff95d\", \n",
    "                  \"Green\": \"#6ab023\", \n",
    "                  \"UKIP\": \"#70147a\", \n",
    "                  \"DUP\":\"#d46a4c\", \n",
    "                  \"Non-Aligned\": \"#ffffff\", \n",
    "                  \"LibDem\": \"#fdbb30\"}\n",
    "    try:\n",
    "        os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "        print(\"File location found\")\n",
    "        classifier = pickle.load(open(\"ModelB\", 'rb'))\n",
    "        print(\"Pickle Model Loaded\")\n",
    "        BritishDF = pandas.DataFrame.from_csv(\"BritishCleaned.csv\")\n",
    "        print(\"CSV loaded\")\n",
    "        vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "        print(\"Vector built\")\n",
    "        BritishDF['text'] = BritishDF['text'].values.astype('U')\n",
    "        vectorizer.fit(BritishDF['text'].values.astype('U'))\n",
    "        print(\"Data fit\")\n",
    "        sentences = BritishDF['text'].values\n",
    "        y = BritishDF['Alignment'].values\n",
    "\n",
    "        sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "            sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(sentences_train)\n",
    "\n",
    "        X_train = vectorizer.transform(sentences_train)\n",
    "        X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "        score = classifier.score(X_test, y_test)\n",
    "\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        os.chdir(\"E:\\Honours\\Master\\Twitter-Learning-Data\\Collections\")\n",
    "\n",
    "        BritishDF = pandas.DataFrame.from_csv(\"AmericanCollection.csv\")\n",
    "\n",
    "        BritishDF['text'] = BritishDF['text'].apply(lambda row: clean_text(row))\n",
    "\n",
    "        vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "\n",
    "        vectorizer.fit(BritishDF['text'])\n",
    "\n",
    "        sentences = BritishDF['text'].values\n",
    "        y = BritishDF['Alignment'].values\n",
    "\n",
    "        sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "            sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(sentences_train)\n",
    "\n",
    "        X_train = vectorizer.transform(sentences_train)\n",
    "        X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        score = classifier.score(X_test, y_test)\n",
    "\n",
    "        print(\"Accuracy:\", score)\n",
    "\n",
    "        os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "        pickle.dump(classifier, open(\"ModelB\", 'wb'))\n",
    "        BritishDF.to_csv(\"BritishCleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load American\n",
    "def LoadAmerican():\n",
    "    global Mode \n",
    "    global vectorizer\n",
    "    global classifier\n",
    "    global PartyColours \n",
    "    Mode = \"American\"\n",
    "    print(\"Loading Mode: \" + Mode) \n",
    "    PartyColours = {\"Republican\": \"#ff0000\", \n",
    "                  \"Democratic\": \"#0015BC\", \n",
    "                  \"Non-Aligned\": \"#ffffff\"}\n",
    "    try:\n",
    "        os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "        print(\"File location found\")\n",
    "        classifier = pickle.load(open(\"ModelA\", 'rb'))\n",
    "        print(\"Pickle Model Loaded\")\n",
    "        AmericanDF = pandas.DataFrame.from_csv(\"AmericanCleaned.csv\")\n",
    "        print(\"CSV loaded\")\n",
    "        vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "        print(\"Vector built\")\n",
    "        AmericanDF['text'] = AmericanDF['text'].values.astype('U')\n",
    "        vectorizer.fit(AmericanDF['text'].values.astype('U'))\n",
    "        print(\"Data fit\")\n",
    "        sentences = AmericanDF['text'].values\n",
    "        y = AmericanDF['Alignment'].values\n",
    "\n",
    "        sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "            sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(sentences_train)\n",
    "\n",
    "        X_train = vectorizer.transform(sentences_train)\n",
    "        X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "        score = classifier.score(X_test, y_test)\n",
    "\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        os.chdir(\"E:\\Honours\\Master\\Twitter-Learning-Data\\Collections\")\n",
    "\n",
    "        AmericanDF = pandas.DataFrame.from_csv(\"AmericanCollection.csv\")\n",
    "\n",
    "        AmericanDF['text'] = AmericanDF['text'].apply(lambda row: clean_text(row))\n",
    "\n",
    "        vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "\n",
    "        vectorizer.fit(AmericanDF['text'])\n",
    "\n",
    "        sentences = AmericanDF['text'].values\n",
    "        y = AmericanDF['Alignment'].values\n",
    "\n",
    "        sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "            sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "        vectorizer = CountVectorizer()\n",
    "        vectorizer.fit(sentences_train)\n",
    "\n",
    "        X_train = vectorizer.transform(sentences_train)\n",
    "        X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(X_train, y_train)\n",
    "        score = classifier.score(X_test, y_test)\n",
    "\n",
    "        print(\"Accuracy:\", score)\n",
    "\n",
    "        os.chdir(\"E:\\Honours\\Master\\Models\")\n",
    "        pickle.dump(classifier, open(\"ModelA\", 'wb'))\n",
    "        AmericanDF.to_csv(\"AmericanCleaned.csv\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Raw\")\n",
    "\n",
    "for filename in os.listdir():\n",
    "        if \".csv\" in filename: \n",
    "            AnalyseUser(filename)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Speeches\")\n",
    "for filename in os.listdir():\n",
    "        if \".txt\" in filename: \n",
    "            AnalyseSpeech(filename)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mode: British\n",
      "Error\n"
     ]
    },
    {
     "ename": "WindowsError",
     "evalue": "[Error 3] The system cannot find the path specified: 'E:\\\\Honours\\\\Master\\\\Twitter-Learning-Data\\\\Collections'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWindowsError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0afd3bc89aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Analyse user test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mLoadBritish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mAnalyseUser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"wildfireone\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-2c9b276f4e3d>\u001b[0m in \u001b[0;36mLoadBritish\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\Honours\\Master\\Twitter-Learning-Data\\Collections\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mBritishDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AmericanCollection.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWindowsError\u001b[0m: [Error 3] The system cannot find the path specified: 'E:\\\\Honours\\\\Master\\\\Twitter-Learning-Data\\\\Collections'"
     ]
    }
   ],
   "source": [
    "#Analyse user test\n",
    "LoadBritish()\n",
    "AnalyseUser(\"wildfireone\", Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#AnalyseSpeech Test\n",
    "AnalyseSpeech(\"WordList\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American\n",
      "File location found\n",
      "Pickle Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded\n",
      "Vector built\n",
      "Data fit\n",
      "American\n",
      "File location found\n",
      "Pickle Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning:\n",
      "\n",
      "from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded\n",
      "Vector built\n",
      "Data fit\n",
      "British\n"
     ]
    }
   ],
   "source": [
    "#MultiMode Test\n",
    "LoadAmerican()\n",
    "AnalyseSpeech(\"Hitler-Danzig\", Mode)\n",
    "\n",
    "LoadBritish()\n",
    "AnalyseSpeech(\"Hitler-Danzig\", Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Non-Aligned'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Single word test\n",
    "x = vectorizer.transform([\"Indepedence!\"])\n",
    "classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#Find Data\n",
    "Username = \"ZoeJP96\"\n",
    "os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Classifications\")\n",
    "try:\n",
    "    os.mkdir(Username)\n",
    "except:\n",
    "    pass\n",
    "os.chdir(Username)\n",
    "\n",
    "Username = Username.replace(\".csv\", \"\")\n",
    "UserData = pandas.DataFrame.from_csv(Username + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1107390126987059200                            agirlcalledlina ellienich\n",
       "1105981012310482946    rt theglasgowcup xfxfxfxxfxfxfxbcxexxdxexxxefx...\n",
       "1105519018822242311                                               bradyy\n",
       "1105518834943954944    rt bradyy creas zoejp got told say hi silent f...\n",
       "1105518566630146049    rt bradyy minut spare pleas fill thi survey he...\n",
       "1104793018379526145    elouisee pryceconni samanthahawk everi hall pa...\n",
       "1102275664177233922                              oldi xfxfxxe pure steam\n",
       "1102150607199260673                         bedroomflick ellienich savag\n",
       "1101490205209575424    rosskylejohnson wehden supposedli pay xcxa ana...\n",
       "1101254830876737536                           hedgehogclown twitter user\n",
       "1101175538947948544    rt jimwaterson coverag momo challeng possibl i...\n",
       "1100844860473528321    xfxfxaxa ixexxm current write honour dissert m...\n",
       "1100128616594239490    rt fkajack xexxci boyfriend told ixexxd never ...\n",
       "1099648712320651270                                   buy new one xfxfxx\n",
       "1097242193075556352    rt linseyhanna femal pl help gal fill dissert ...\n",
       "1095035860695896064                          stephxliz ellienich lauraev\n",
       "1094015560868941825    rt chloelstok wonder mani girl arenxexxt happi...\n",
       "1093293797251973120    instagram dietdetox product allow sold celebr ...\n",
       "1091783493191942146                             gabbiejarvi samanthahawk\n",
       "1091770516669718529      milk lauraxexx dress xexcxa hummingbird glasgow\n",
       "1091659543627644928                          rt euancampbel loui theroux\n",
       "1088503176574177281    rt bbcphilipsim charg alex salmond face multip...\n",
       "1088060624473636869                               dancingbratz ellienich\n",
       "1088004417494892544    rt hevarowan manag made pickl busi mascot get ...\n",
       "1085973591802503168      samanthahawk tmbirpic pryceconni pretti xfxfxxd\n",
       "1085205443104960512    rt spidervesr thi use brand thi engag audienc ...\n",
       "1085126309192392704               anesuishec lauraev take rubbish xfxfxx\n",
       "1083500627848818690    rt jodykerr need see thi today crazi deceiv in...\n",
       "1083314876750151680    rt kdownonit confirm thi guy ask could xexxcwa...\n",
       "1082948677814177792              bedroomflick ellienich pleas thi xfxfxx\n",
       "                                             ...                        \n",
       "648264286469533701     rt bradyy journo groupchat best thing ever ask...\n",
       "648235173427998721                    oooooh night dont worri bout thing\n",
       "648234964459327488     rt gingylock mammi daddi actual smell bank hol...\n",
       "648160025807032320         pryceconni must someth bainfield water xfxfxx\n",
       "648118204859068416     fresher flu ha final hit lie bed chicken tikka...\n",
       "647837117863829504     good two day visit main edinburgh hoe xfxfxxad...\n",
       "647802261222096896     watch comic con goer wander around glasgow cos...\n",
       "647725164482924544     rt laauraaa girl fav eachoth tweet friend u dm...\n",
       "647712012265476096              conni glaswegian ask mhairi black xfxfxx\n",
       "647370596058316800     sit starbuck edinburgh thi happen xfxfxxsamant...\n",
       "647324144485433344           pryceconni sodamntru aimeeeebethh ye xfxfxx\n",
       "647157285303791617                                        got intoxicaaa\n",
       "647114220769832961                        dolmio stirin sauc life xfxfxx\n",
       "647097186606518272     pictur way real anyon use scotrail scotfail vi...\n",
       "647095749671583744                           bradyy hahaha xexxbaxefxbxf\n",
       "647078931460440064     mani thank glasgowcc love bank holiday weekend...\n",
       "647077195312144384     rt greaterglasgpol polic respond report man gu...\n",
       "646450915059351552     room right next kitchen mean engulf sweet smel...\n",
       "646083989351079936     rt cameronpig davidcameron wa one time thing s...\n",
       "646048074993958912                                  pryceconni soznewlif\n",
       "646047809393827841                          pryceconni say thi past week\n",
       "646025754103427072     shorthand make tear hair day one xfxfxxdxfxfxfxbb\n",
       "645754266922156032     hope piggat consid legitim excus tire lectur t...\n",
       "645750053928370178     rt jimwaterson ten hour prime minist spokesper...\n",
       "645723924286865409     rt jamieross fifteen minut netflix chill give ...\n",
       "645677913220771841                     meganbaxt good know live interest\n",
       "645672334611881984     whi move away get mani friend request peopl sc...\n",
       "645563561784311808                     shake like polaroid pictur xexcxa\n",
       "645390611055517696                              linseyhanna xfxfxxxfxfxx\n",
       "645294515285270528     reason flat parti peopl put beer freezer forge...\n",
       "Name: cleantext, Length: 2981, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserData['cleantext'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "UserData['cleantext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'RT @carterjwm: HELP ME PLEASE. A MAN NEEDS HIS NUGGS https://t.co/4SrfHmEMo3'\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Retweets\n",
    "def Smallclean(text, ):\n",
    "    text = TweetbScrub(text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z@1-9 ]', '', text)\n",
    "    return text\n",
    "\n",
    "def FindTop(UserData):\n",
    "    \n",
    "    UserData = UserData[~UserData.text.str.contains(\"RT \")]\n",
    "    favourite = UserData.loc[UserData['favorite_count'].idxmax()]\n",
    "    retweet = UserData.loc[UserData['retweet_count'].idxmax()]\n",
    "    file = open(\"Favourite.txt\",\"w\") \n",
    "    file.writelines(favourite['text'] + \"\\n\")\n",
    "    file.writelines(str(favourite['favorite_count']))\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"Retweet.txt\",\"w\") \n",
    "    file.writelines(retweet['text'] + \"\\n\")\n",
    "    file.writelines(str(retweet['retweet_count']))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"Favourite.txt\",\"w\") \n",
    "file1.writelines(favourite['text'] + \"\\n\")\n",
    "file1.writelines(str(favourite['favorite_count']))\n",
    "file1.close()\n",
    "                 \n",
    "file1 = open(\"Retweet.txt\",\"w\") \n",
    "file1.writelines(retweet['text'] + \"\\n\")\n",
    "file1.writelines(str(retweet['retweet_count']))\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "file1 = open(\"Favourite.txt\",\"a\") \n",
    "tweet['text'] = TweetbScrub(tweet['text'])\n",
    "file1.write(retweet['text'])\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-24b82f91e253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mUserData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUsername\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-Tweets-Classification.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mGeneratePie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUsercollection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUsername\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"-Tweets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mMode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"British\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-5b582847d066>\u001b[0m in \u001b[0;36mGeneratePie\u001b[1;34m(collection, name)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mGeneratePie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_credentials_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musername\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'itjallingii'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hiFqLEhfwSwdDlsj9lT8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "Username = \"wildfireone\"\n",
    "os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Classifications\")\n",
    "try:\n",
    "    os.mkdir(Username)\n",
    "except:\n",
    "    pass\n",
    "os.chdir(Username)\n",
    "\n",
    "#get_all_tweets(Username)\n",
    "Username = Username.replace(\".csv\", \"\")\n",
    "UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "FindTop(UserData)\n",
    "#Vectorize the text\n",
    "\n",
    "UserData['CleanText'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "Sentences = vectorizer.transform(UserData['CleanText'])\n",
    "#Perform prediction of text inputted\n",
    "Classification = classifier.predict(Sentences)\n",
    "#Output a basic overview of the predictions\n",
    "#Append original data with model classification\n",
    "UserData[\"Classification\"] = Classification\n",
    "UserDataSentiment = UserData\n",
    "\n",
    "Usercollection = collections.Counter(UserData[\"Classification\"])\n",
    "w = csv.writer(open(\"output.csv\", \"w\"))\n",
    "for key, val in Usercollection.items():\n",
    "        w.writerow([key, val])\n",
    "UserData.to_csv(Username + \"-Tweets-Classification.csv\", encoding='utf-8')\n",
    "GeneratePie(Usercollection, Username + \"-Tweets\")\n",
    "\n",
    "if Mode == \"British\":\n",
    "    UserDataSentiment = UserDataSentiment.apply(SentimentclassiferBritish,axis=1)\n",
    "else:\n",
    "    UserDataSentiment = UserDataSentiment.apply(SentimentclassiferAmerican,axis=1)\n",
    "UserData.drop('CleanText', axis=1, inplace=True)\n",
    "UserDataSentiment.drop('CleanText', axis=1, inplace=True)\n",
    "UsercollectionSentiment = collections.Counter(UserDataSentiment[\"Classification\"])\n",
    "#Change directory to the classifications directory for saving\n",
    "#Save Classifications to classificaiton directory\n",
    "\n",
    "UserData.to_csv(Username + \"-Tweets-Classification.csv\", encoding='utf-8')\n",
    "GeneratePie(Usercollection, Username + \"-Tweets\")\n",
    "UserDataSentiment.to_csv(Username + \"-Tweets-Classification-S.csv\", encoding='utf-8')\n",
    "if Mode == \"British\":\n",
    "    GeneratePieS(UsercollectionSentiment, Username + \"-Tweets-S\")\n",
    "else:\n",
    "    GeneratePie(UsercollectionSentiment, Username + \"-Tweets-S\")\n",
    "\n",
    "os.remove(Username + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Non-Aligned': 1936,\n",
       "         'SNP': 512,\n",
       "         'Conservative': 157,\n",
       "         'Labour': 226,\n",
       "         'UKIP': 63,\n",
       "         'LibDem': 137,\n",
       "         'DUP': 30,\n",
       "         'Green': 136})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Usercollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = csv.writer(open(\"output.csv\", \"w\"))\n",
    "for key, val in Usercollection.items():\n",
    "        w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
