{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, os, collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "os.chdir(\"e:\\Honours\\Master\\PythonAnalysis\\Sentiment-Topics\")\n",
    "\n",
    "RepublicanTopics = open(\"Republican.txt\").read()\n",
    "DemocraticTopics = open(\"Democratic.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FoxNews', 'Washingtontimes', 'WallStreetJournal', 'NationalReview', 'Capitalism', 'Conservatism', 'RonaldReagan', 'MargaretThatcher', 'GeorgeWashington', 'WinstonChurchill', 'GeorgeW.Bush', 'MitchMcConnell', 'RupertMurdoch', 'RushLimbaugh', 'TonyAbbott', 'MittRomney', 'DonaldTrump', 'Gun', 'GunControl', 'IRA', 'InternationalRifleAssosiation', 'Abortion', 'StemCell', 'HomeSchooling', 'PrivateEducation', 'BorderSecurity', 'Wall', 'Walls', 'UniversalHealthcare', 'Healthcare', 'Conservative', 'BalancedBudget', 'ReducedGovernmentSpending']\n"
     ]
    }
   ],
   "source": [
    "RepublicanTopics = RepublicanTopics.replace(\" \", \"\")\n",
    "RepublicanTopics = RepublicanTopics.split(\",\")\n",
    "\n",
    "print(RepublicanTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans, and people of the world, thank you', ' We the citizens of America are now joined in a great national effort to rebuild our country and restore its promise for all of our people', ' Together we will determine the course of America, and the world, for many, many years to come', ' We will face challenges', ' We will confront hardships, but we will get the job done', '\\n\\nEvery four years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition', ' They have been magnificent', ' Thank you', \"\\n\\nToday's ceremony, however, has very special meaning, because today we are not merely transferring power from one administration to another, or from one party to another, but we are transferring power from Washington, D\", 'C', ', and giving it back to you, the people', \"\\n\\nFor too long, a small group in our nation's capital has reaped the rewards of government, while the people have borne the cost\", ' Washington flourished, but the people did not share in its wealth', ' Politicians prospered, but the jobs left and the factories closed', ' The establishment protected itself, but not the citizens of our country', ' Their victories have not been your victories', \" Their triumphs have not been your triumphs, and while they celebrated in our nation's capital, there was little to celebrate for struggling families all across our land\", ' That all changes, starting right here and right now, because this moment is your moment --- it belongs to you', ' It belongs to everyone gathered here today, and everyone watching, all across America', ' This is your day', ' This is your celebration, and this, the United States of America, is your country', '\\n\\nWhat truly matters is not which party controls our government, but whether our government is controlled by the people', ' January 20th, 2017 will be remembered as the day the people became the rulers of this nation again', ' The forgotten men and women of our country, will be forgotten no longer', ' Everyone is listening to you now', ' You came by the tens of millions to become part of a historic movement, the likes of which the world has never seen before', ' At the center of this movement is a crucial conviction, that a nation exists to serve its citizens', ' Americans want great schools for their children, safe neighborhoods for their families, and good jobs for themselves', ' These are just and reasonable demands of righteous people and a righteous public, but for too many of our citizens a different reality exists', ' Mothers and children trapped in poverty in our inner cities, rusted out factories, scattered like tombstones across the across the landscape of our nation, an education system flush with cash, but which leaves our young and beautiful students deprived of all knowledge, and the crime, and the gangs, and the drugs that have stolen too many lives and robbed our country of so much unrealized potential', ' This American carnage stops right here and stops right now', '\\n\\nWe are one nation and their pain is our pain', ' Their dreams are our dreams and their success will be our success', ' We share one heart, one home, and one glorious destiny', ' The oath of office, I take today, is an oath of allegiance to all Americans', \" For many decades, we've enriched foreign industry at the expense of American industry, subsidized the armies of other countries, while allowing for the very sad depletion of our military\", \" We've defended other nation's borders while refusing to defend our own\", \" And spent trillions and trillions of dollars overseas, while America's infrastructure has fallen into disrepair and decay\", \" We've made other countries rich while the wealth, strength and confidence of our country has dissipated over the horizon\", ' One by one, the factories shuddered and left our shores, with not even a thought about the millions and millions of American workers that were left behind', ' The wealth of our middle class has been ripped from their homes and then redistributed all across the world', '\\n\\nBut that is the past, and now we are looking only to the future', \" We assembled here today our issuing a new decree to be heard in every city, in every foreign capital, and in every hall of power, from this day forward: a new vision will govern our land, from this day forward, it's going to be only America first\", ' America first', '\\n\\nEvery decision on trade, on taxes, on immigration, on foreign affairs will be made to benefit American workers and American families', ' We must protect our borders from the ravages of other countries making our products, stealing our companies and destroying our jobs', ' Protection will lead to great prosperity and strength', ' I will fight for you with every breath in my body, and I will never, ever let you down', ' America will start winning again, winning like never before', ' We will bring back our jobs', ' We will bring back our borders', ' We will bring back our wealth, and we will bring back our dreams', ' We will build new roads and highways and bridges and airports and tunnels, and railways, all across our wonderful nation', ' We will get our people off of welfare and back to work, rebuilding our country with American hands and American labor', '\\n\\nWe will follow two simple rules: buy American, and hire American', ' We will seek friendship and goodwill with the nations of the world, but we do so with the understanding that it is the right of all nations to put their own interests first', ' We do not seek to impose our way of life on anyone, but rather to let it shine as an example', ' We will shine for everyone to follow', ' We will reinforce old alliances and form new ones, and you unite the civilized world against radical Islamic terrorism, which we will eradicate completely from the face of the Earth', '\\n\\nAt the bedrock of our politics will be a total allegiance to the United States of America, and through our loyalty to our country, we will rediscover our loyalty to each other', ' When you open your heart to patriotism, there is no room for prejudice', \" The Bible tells us, how good and pleasant it is when God's people live together in unity\", ' We must speak our minds openly, debate our disagreements, but always pursue solidarity', ' When America is united, America is totally unstoppable', ' There should be no fear', ' We are protected, and we will always be protected', ' We will be protected by the great men and women of our military and law enforcement', ' And most importantly, we will be protected by God', '\\n\\nFinally, we must think big and dream even bigger', ' In America, we understand that a nation is only living as long as it is striving', ' We will no longer accept politicians who are all talk and no action, constantly complaining but never doing anything about it', ' The time for empty talk is over', ' Now arrives the hour of action', ' Do not allow anyone to tell you that it cannot be done', ' No challenge can match the heart and fight and spirit of America', ' We will not fail', ' Our country will thrive and prosper again', '\\n\\nWe stand at the birth of a new millennium, ready to unlock the mysteries of space, to free the Earth from the miseries of disease and to harness the industries and technologies of tomorrow', ' A new national pride will stir our souls, lift our sights and heal our divisions', \" It's time to remember that old wisdom our soldiers will never forget, that whether we are black, or brown, or white, we all bleed the same red blood of patriots\", ' We all enjoy the same glorious freedoms, and we all salute the same, great American flag', ' And whether a child is born in the urban sprawl of Detroit or the windswept plains of Nebraska, they look up at the at the same night sky, they fill their heart with the same dreams and they are infused with the breath of life by the same almighty creator', '\\n\\nSo to all Americans, in every city near and far, small and large, from mountain to mountain, from ocean to ocean, hear these words', ' You will never be ignored again', ' Your voice, your hopes, and your dreams will define our American destiny', ' And your courage and goodness and love, will forever guide us along the way', ' Together, we will make America strong again', ' We will make America wealthy again', ' We will make America proud again We will make America safe again, And yes, together, we will make we will make America great again', ' Thank you', ' God bless you', ' And god bless America', ' Thank you', ' God bless America', ' ']\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "os.chdir(\"E:\\Honours\\Master\\Speeches\\Republican\\Raw\")\n",
    "\n",
    "speech = open(\"Trump-Inauguration.txt\").read()\n",
    "collection = speech.split(\".\")\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EmoticonTransfer(text, ):\n",
    "    sad_emoticons = [\":-(\", \":(\", \":-|\", \";-(\", \";-<\", \"|-{\"]\n",
    "    happy_emoticons = [\":-)\", \":)\", \":o)\", \":-}\", \";-}\", \":->\", \";-)\"]\n",
    "    borderchecker = [\"(\",\")\"]\n",
    "    if text in happy_emoticons:\n",
    "        return \"happy\"\n",
    "    if text in sad_emoticons:\n",
    "        return \"sad\"\n",
    "    else: \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.51599999999988\n",
      "50.82099999999998\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Change directory to the raw data for testing\n",
    "os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Raw\")\n",
    "#Import user data\n",
    "Username = \"john\"\n",
    "UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "tweets = UserData['text'].tolist()\n",
    "\n",
    "UserData['Party'] = \"NA\"\n",
    "UserData['Sentiment'] = 0\n",
    "\n",
    "RepublicanSentiment = 0\n",
    "DemocraticSentiment = 0\n",
    "DemocraticTweets = 0\n",
    "RepublicanTweets = 0\n",
    "\n",
    "\n",
    "for sentence in tweets:\n",
    "    words = sentence.split()\n",
    "    RepublicanMentions = 0\n",
    "    DemocraticMentions = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            if re.search(word, DemocraticTopics) and re.search(word, RepublicanTopics):\n",
    "                pass\n",
    "                #print(\"Both\")\n",
    "            elif re.search(word, RepublicanTopics):\n",
    "                #print(\"Republican\")\n",
    "                RepublicanMentions += 1\n",
    "\n",
    "            elif re.search(word, DemocraticTopics):\n",
    "                #print(\"Democratic\")\n",
    "                DemocraticMentions += 1\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"Neither\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if RepublicanMentions > DemocraticMentions:\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        RepublicanSentiment += sentiment.get(\"compound\")\n",
    "        RepublicanTweets += 1\n",
    "    elif DemocraticMentions > RepublicanMentions:\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        DemocraticSentiment += sentiment.get(\"compound\")\n",
    "        DemocraticTweets += 1\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        #print(\"Nothing determined for sentence: \", sentence)\n",
    "print(DemocraticSentiment)\n",
    "print(RepublicanSentiment)\n",
    "print(DemocraticMentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Republican.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fba6f92fd6ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mRepublicanTopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Republican.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mDemocraticTopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Democratic.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Republican.txt'"
     ]
    }
   ],
   "source": [
    "RepublicanTopics = open(\"Republican.txt\").read()\n",
    "DemocraticTopics = open(\"Democratic.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every four years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition \n",
      "0.8689\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "text = \"Every four years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition\"\n",
    "text = text.split()\n",
    "finished = \"\"\n",
    "for text in text:\n",
    "    finished += text + \" \"\n",
    "score = sid.polarity_scores(finished)\n",
    "print(str(finished))\n",
    "print(score.get(\"compound\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import unicodedata\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z ]', '', text)\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = stem_text(text) # stemming\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Change directory to the raw data for testing\n",
    "os.chdir(\"e:\\Honours\\Master\\Twitter-User-Data\\Raw\")\n",
    "#Import user data\n",
    "Username = \"john\"\n",
    "UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "\n",
    "UserData['text'].tolist()\n",
    "\n",
    "UserData['Party'] = \"NA\"\n",
    "UserData['Sentiment'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def Tweetclassifer(tweet, ):\n",
    "    sentence = tweet['text']\n",
    "    words = sentence.split()\n",
    "    RepublicanMentions = 0\n",
    "    DemocraticMentions = 0\n",
    "    DemocraticSentiment = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            if re.match(word, DemocraticTopics) and re.search(word, RepublicanTopics):\n",
    "                pass\n",
    "                #print(\"Both\")\n",
    "            elif re.match(word, RepublicanTopics):\n",
    "                #print(\"Republican\")\n",
    "                print(\"Martch for Republican Topics with \", word)\n",
    "                RepublicanMentions += 1\n",
    "\n",
    "            elif re.match(word, DemocraticTopics):\n",
    "                #print(\"Democratic\")\n",
    "                print(\"Match for Democratic Topics with \", word)\n",
    "                DemocraticMentions += 1\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"Neither\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if RepublicanMentions > DemocraticMentions:\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        tweet['Party'] = \"Republican\"\n",
    "        print(\"Republican\")\n",
    "        return pandas.Series([\"Republican\", sentiment.get(\"compound\")])\n",
    "        \n",
    "        \n",
    "    elif DemocraticMentions > RepublicanMentions:\n",
    "        sentiment = sid.polarity_scores(sentence)\n",
    "        tweet['Party'] = \"Democratic\"\n",
    "        tweet['Sentiment'] = sentiment.get(\"compound\")\n",
    "        print(\"Democratic\")\n",
    "        return pandas.Series([\"Democratic\", sentiment.get(\"compound\")])\n",
    "        \n",
    "    else:\n",
    "        return pandas.Series([\"NA\", 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#Change directory to the raw data for testing\n",
    "os.chdir(\"e:\\Honours\\Master\\Twitter-User-Data\\Raw\")\n",
    "#Import user data\n",
    "Username = \"john\"\n",
    "UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UserData['text'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "UserData[['Party', 'Sentiment']] = UserData.apply(lambda row: Tweetclassifer(row), axis=1)\n",
    "UserData.to_csv(\"JohnTest-Classification.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UserData.to_csv(\"JohnTest-Classification.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fox News, Washington times, Wall Street Journal, National Review, Capitalism, Conservatism, Ronald Reagan, Margaret Thatcher, George Washington, Winston Churchill, George W. Bush, Mitch McConnell, Rupert Murdoch, Rush Limbaugh, Tony Abbott, Mitt Romney, Donald Trump, Gun, Gun Control, IRA, International Rifle Assosiation, Stem Cell, Home Schooling, Private Education, Border Security, Wall, Walls, Universal Healthcare, Healthcare, Conservative, Balanced Budget, Reduced Government Spending\n"
     ]
    }
   ],
   "source": [
    "print(RepublicanTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e9aa0bd592ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0munidecode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_non_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "def remove_non_ascii(text):\n",
    "    return unidecode(unicode(text, encoding = \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a3a41c1d9718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_non_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nickw\\\\Anaconda3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters=string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = stem_text(text) # stemming\n",
    "    text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
