{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import checks\n",
    "from pip._internal import main as pipmain\n",
    "\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "except:\n",
    "    pipmain([\"install\", \"nltk\"])\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "try:\n",
    "    import re\n",
    "except:\n",
    "    pipmain([\"install\", \"re\"])\n",
    "    import re\n",
    "try:\n",
    "    import string\n",
    "except:\n",
    "    pipmain([\"install\", \"string\"])\n",
    "    import string\n",
    "try:\n",
    "    import unicodedata\n",
    "except:\n",
    "    pipmain([\"install\", \"unicodedata\"])\n",
    "    import unicodedata\n",
    "try:\n",
    "    import pandas\n",
    "except:\n",
    "    pipmain([\"install\", \"pandas\"])\n",
    "    import pandas\n",
    "try:\n",
    "    import os\n",
    "except:\n",
    "    pipmain([\"install\", \"os\"])\n",
    "    import os\n",
    "try:\n",
    "    import collections\n",
    "except:\n",
    "    pipmain([\"install\", \"collections\"])\n",
    "    import collections\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "except:\n",
    "    pipmain([\"install\", \"sklearn\"])\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "try:\n",
    "    import tweepy\n",
    "except:\n",
    "    pipmain([\"install\", \"tweepy\"])\n",
    "    import tweepy\n",
    "try:\n",
    "    import csv\n",
    "except:\n",
    "    pipmain([\"install\", \"csv\"])\n",
    "    import csv\n",
    "try:\n",
    "    import time\n",
    "except:\n",
    "    pipmain([\"install\", \"time\"])\n",
    "    import time\n",
    "try:\n",
    "    import pickle\n",
    "except:\n",
    "    pipmain([\"install\", \"pickle\"])\n",
    "    import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # or LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "import unicodedata\n",
    "default_stemmer = PorterStemmer()\n",
    "default_stopwords = stopwords.words('english') # or any other list of your choice\n",
    "def TweetbScrub(text, ):\n",
    "    if text[0] == \"b\":\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def clean_text(text, ):\n",
    "\n",
    "    def tokenize_text(text):\n",
    "        return [w for s in sent_tokenize(text) for w in word_tokenize(s)]\n",
    "\n",
    "    def remove_special_characters(text, characters = string.punctuation.replace('-', '')):\n",
    "        tokens = tokenize_text(text)\n",
    "        pattern = re.compile('[{}]'.format(re.escape(characters)))\n",
    "        return ' '.join(filter(None, [pattern.sub('', t) for t in tokens]))\n",
    "\n",
    "    def stem_text(text, stemmer=default_stemmer):\n",
    "        tokens = tokenize_text(text)\n",
    "        return ' '.join([stemmer.stem(t) for t in tokens])\n",
    "\n",
    "    def remove_stopwords(text, stop_words=default_stopwords):\n",
    "        tokens = [w for w in tokenize_text(text) if w not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^^a-zA-Z ]', '', text)\n",
    "    text = text.strip(' ') # strip whitespaces\n",
    "    text = text.lower() # lowercase\n",
    "    text = stem_text(text) # stemming\n",
    "    try:\n",
    "        text = remove_special_characters(text) # remove punctuation and symbols\n",
    "    except:\n",
    "        print(\"Error\")\n",
    "    text = remove_stopwords(text) # remove stopwords\n",
    "    #text.strip(' ') # strip whitespaces again?\n",
    "\n",
    "    return text\n",
    "\n",
    "PartyColours = {\"Labour\": \"#dc241f\", \n",
    "              \"Conservative\": \"#0087dc\", \n",
    "              \"SNP\": \"#fff95d\", \n",
    "              \"Green\": \"#6ab023\", \n",
    "              \"UKIP\": \"#70147a\", \n",
    "              \"DUP\":\"#d46a4c\", \n",
    "              \"Non-Aligned\": \"#ffffff\", \n",
    "              \"LibDem\": \"#fdbb30\"}\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = \"TC98w89JxQK2v4vPEqLLxJLx0\"\n",
    "consumer_secret = \"le4t2JCgoT3CBZwToaKdOJx5LFYDDkGL5e3Pjl2ZtfTqYV46Fs\"\n",
    "access_key = \"4459846396-tU9aYf4E5r9eHhJnniU7OsyrLNJhzEd4cpVeFFe\"\n",
    "access_secret = \"UaY6kpdXbdV7cAsAxrKLzFTkKSLtW8dcNTe1CYniUl6xM\"\n",
    "\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []\n",
    "\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print(\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv\n",
    "\n",
    "\n",
    "\n",
    "    outtweets = []\n",
    "    for tweet in alltweets:\n",
    "        media_count = 0\n",
    "        url_count =0\n",
    "        if \"media\" in tweet.entities:\n",
    "            media_count =1\n",
    "        if \"urls\" in tweet.entities:\n",
    "            url_count =len(tweet.entities[\"urls\"])\n",
    "        outtweets.append([tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"),tweet.retweet_count,tweet.favorite_count,media_count,url_count])\n",
    "\n",
    "    #write the csv\n",
    "    with open(screen_name + '.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\",\"created_at\",\"text\",\"retweet_count\",\"favorite_count\",\"media_count\",\"url_count\"])\n",
    "        writer.writerows(outtweets)\n",
    "\n",
    "    pass\n",
    "\n",
    "def GeneratePie(collection, name):\n",
    "    import plotly \n",
    "    plotly.tools.set_credentials_file(username='andriy93', api_key='j4UgdivFmCTlwkO1bYWS')\n",
    "\n",
    "    import plotly.plotly as py\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "    labels = list(collection.keys())\n",
    "    values = list(collection.values())\n",
    "    colors = [PartyColours[i] for i in labels]\n",
    "\n",
    "\n",
    "    trace = go.Pie(labels=labels, values=values,\n",
    "                   hoverinfo='label+percent', textinfo='value', \n",
    "                   textfont=dict(size=10),\n",
    "                   marker=dict(colors=colors, \n",
    "                               line=dict(color='#000000', width=2)))\n",
    "\n",
    "    data = [trace]\n",
    "    layout = go.Layout(title=name)\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "\n",
    "    # Save the figure as a png image:\n",
    "    py.image.save_as(fig, name + '.png')\n",
    "\n",
    "\n",
    "def AnalyseSpeech(Filename):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Speeches\")\n",
    "    Filename = Filename.replace(\".txt\", \"\")\n",
    "    Speech = open(Filename + \".txt\").read()\n",
    "    Sentence = vectorizer.transform([Speech])\n",
    "    classifier.predict(Sentence)\n",
    "    Speech = Speech.replace(\"\\n\", \"\")\n",
    "    Speech = Speech.split(\".\")\n",
    "    for sentence in Speech:\n",
    "        if sentence == \" \":\n",
    "            Speech.remove(sentence)\n",
    "    Speech = list(filter(None, Speech))\n",
    "\n",
    "    Speech = pandas.DataFrame(Speech)\n",
    "    Speech[\"Classification\"] = \"\"\n",
    "    Speech.columns = ['Text', \"Classification\"]\n",
    "    Speech['CleanText'] = Speech['Text'].apply(lambda row: clean_text(row))\n",
    "    Sentence = vectorizer.transform(Speech[\"CleanText\"])\n",
    "    Classification = classifier.predict(Sentence)\n",
    "    Speech[\"Classification\"] = Classification\n",
    "    UserData = UserData.apply(Sentimentclassifer,axis=1)\n",
    "    Speech.drop('CleanText', axis=1, inplace=True)\n",
    "    Speechcollection = collections.Counter(Classification)\n",
    "    #Change directory to the classifications directory for saving\n",
    "    os.chdir(\"E:\\Honours\\Master\\Speeches\\Classifications\\American\")\n",
    "    #Save Classifications to classificaiton directory\n",
    "    try:\n",
    "        os.mkdir(Filename)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Filename)\n",
    "    Speech.to_csv(Filename + \"Classification.csv\", encoding='utf-8')\n",
    "\n",
    "    GeneratePie(Speechcollection, Filename)\n",
    "\n",
    "def AnalyseUser(Username):\n",
    "    os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Classifications\")\n",
    "    try:\n",
    "        os.mkdir(Username)\n",
    "    except:\n",
    "        pass\n",
    "    os.chdir(Username)\n",
    "    get_all_tweets(Username)\n",
    "    Username = Username.replace(\".csv\", \"\")\n",
    "    UserData = pandas.DataFrame.from_csv(Username + \".csv\")\n",
    "    #Vectorize the text\n",
    "    UserData['CleanText'] = UserData['text'].apply(lambda row: clean_text(row))\n",
    "    Sentences = vectorizer.transform(UserData['CleanText'])\n",
    "    #Perform prediction of text inputted\n",
    "    Classification = classifier.predict(Sentences)\n",
    "    #Output a basic overview of the predictions\n",
    "    #Append original data with model classification\n",
    "    UserData[\"Classification\"] = Classification\n",
    "    UserData = UserData.apply(Sentimentclassifer,axis=1)\n",
    "    UserData.drop('CleanText', axis=1, inplace=True)\n",
    "    Usercollection = collections.Counter(UserData[\"Classification\"])\n",
    "    #Change directory to the classifications directory for saving\n",
    "    #Save Classifications to classificaiton directory\n",
    "    UserData.to_csv(Username + \"-Classification.csv\", encoding='utf-8')\n",
    "    os.remove(Username + \".csv\")\n",
    "    \n",
    "    \n",
    "    #GeneratePie(Usercollection, Username)\n",
    "\n",
    "def Sentimentclassifer(sentence, ):\n",
    "    sentiment = sid.polarity_scores(sentence['CleanText'])\n",
    "    sentence['Sentiment'] = sentiment['compound']\n",
    "    if (sentence['Sentiment'] < -0) & (sentence['Classification'] != \"Non-Aligned\"):\n",
    "        sentence['Classification'] = \"Anti-\" + sentence['Classification']\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8103104405133541\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-af9e8f1e11b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\Honours\\Master\\Model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mBritishDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BritishCollection.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-af9e8f1e11b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\Honours\\Master\\Model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(\"E:\\Honours\\Master\\Model\")\n",
    "    test = pickle.load(open(\"Model\", 'rb'))\n",
    "    BritishDF = pandas.DataFrame.from_csv(\"BritishCleaned.csv\")\n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "    vectorizer.fit(BritishDF['text'])\n",
    "except:\n",
    "    os.chdir(\"E:\\Honours\\Master\\Twitter-Learning-Data\\Collections\")\n",
    "\n",
    "    BritishDF = pandas.DataFrame.from_csv(\"BritishCollection.csv\")\n",
    "\n",
    "    BritishDF['text'] = BritishDF['text'].apply(lambda row: clean_text(row))\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "\n",
    "    vectorizer.fit(BritishDF['text'])\n",
    "\n",
    "    sentences = BritishDF['text'].values\n",
    "    y = BritishDF['Alignment'].values\n",
    "\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "        sentences, y, test_size=0.05, random_state=1000)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences_train)\n",
    "\n",
    "    X_train = vectorizer.transform(sentences_train)\n",
    "    X_test  = vectorizer.transform(sentences_test)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "\n",
    "    print(\"Accuracy:\", score)\n",
    "\n",
    "    os.chdir(\"E:\\Honours\\Master\\Model\")\n",
    "    pickle.dump(classifier, open(\"Model\", 'wb'))\n",
    "    BritishDF.to_csv(\"BritishCleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Twitter-User-Data\\Raw\")\n",
    "\n",
    "for filename in os.listdir():\n",
    "        if \".csv\" in filename: \n",
    "            AnalyseUser(filename)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"E:\\Honours\\Master\\Speeches\")\n",
    "for filename in os.listdir():\n",
    "        if \".txt\" in filename: \n",
    "            AnalyseSpeech(filename)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:GuyFieriisJesus\n",
      "getting tweets before 598091020547583999\n",
      "...32 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:192: FutureWarning:\n",
      "\n",
      "from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uesrname = input(\"Username:\")\n",
    "AnalyseUser(uesrname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 598091020547583999\n",
      "...32 tweets downloaded so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:193: FutureWarning:\n",
      "\n",
      "from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AnalyseUser(\"GuyFieriisJesus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanText</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1102210525885288451</th>\n",
       "      <td>tim cook appl publicli commit work nvidia driv...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100082605477822464</th>\n",
       "      <td>use aframevr amp cesiumj univers honour projec...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008493804297613312</th>\n",
       "      <td>familysimpson notabilityapp explainevrythng</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008492838538096643</th>\n",
       "      <td>familysimpson notabilityapp explainevrythng tr...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963696549674012672</th>\n",
       "      <td>rt shopap new zerok thermal shirt avail amazon...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906166602013573120</th>\n",
       "      <td></td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888146897235582976</th>\n",
       "      <td>secur import plexapp wed like support two fact...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828913387379781632</th>\n",
       "      <td>blackwakegam tjrlz edmcklabour sasrich</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781422598458662912</th>\n",
       "      <td>forzamotorsport game pc onli run min befor cra...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779409641746882560</th>\n",
       "      <td>profbriancox briancoxl loreal head shoulder ki...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751136630572998656</th>\n",
       "      <td>rcommentari get penalti get titan theme</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722300591855308800</th>\n",
       "      <td>rt boxcouk win netgear nighthawk modem router ...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634041443867299840</th>\n",
       "      <td>nerdcubedgam hey dan scoop</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632827417023893504</th>\n",
       "      <td>download bear brows like anoth countri thetunn...</td>\n",
       "      <td>LibDem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629554315288576000</th>\n",
       "      <td>your secur path internet</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598157505416540161</th>\n",
       "      <td>threeuksupport im tenerif feel home destin wan...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558739132282044416</th>\n",
       "      <td>rt hannahinabucket fab lunch catch jennywardx ...</td>\n",
       "      <td>SNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493851355178860545</th>\n",
       "      <td>hannahinabucket robbiethough zoeejadeexo caatw...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492655785043038208</th>\n",
       "      <td>just report jam n use waze social gp</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492647635002871808</th>\n",
       "      <td>just report hazard n use waze social gp</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489132874608967680</th>\n",
       "      <td>just report jam chapel ln blockley use waze so...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476312716895084544</th>\n",
       "      <td>rt rockstargam gtav come ps xbox one amp pc th...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462214159396913152</th>\n",
       "      <td>if ani ani spare time could fill servey colleg...</td>\n",
       "      <td>SNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459266540085706752</th>\n",
       "      <td>rt michellefraas part alway want go back school</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459265882112688130</th>\n",
       "      <td>robbiethough xfxfxdx</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459265657490931713</th>\n",
       "      <td>hannahinabucket onli hand satisfi lamaswithhat</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456728504273346560</th>\n",
       "      <td>rt jalopnik thi dodg challeng</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456447257450000384</th>\n",
       "      <td>tjrlz mrpaul paul thi touchi subject</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456435955562729473</th>\n",
       "      <td>tjrlz disagre</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453645386909904896</th>\n",
       "      <td>rt jensonbutton wa look like strong weekend ba...</td>\n",
       "      <td>Labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164088297604382721</th>\n",
       "      <td>thomasbrown oh realli oo</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164030738533056513</th>\n",
       "      <td>it wet miser cold worst revis exam</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162149913717784576</th>\n",
       "      <td>duncanjmacf inde</td>\n",
       "      <td>UKIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161575214101168128</th>\n",
       "      <td>ey see</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161574225264652289</th>\n",
       "      <td>missamandab youtub nice video scene actual lea...</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161449169662394368</th>\n",
       "      <td>thomasbrown nice coupl</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160664768099790848</th>\n",
       "      <td>good morn oo</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160121368782970880</th>\n",
       "      <td>thomasbrown kirza haha geniu</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158976999761788929</th>\n",
       "      <td></td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157763679486619649</th>\n",
       "      <td>techhub dont work hard</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157763574075363328</th>\n",
       "      <td>rt techhub use googl chrome youll happi know h...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157181188153421824</th>\n",
       "      <td>i nomin prosynd shorti award game becaus boobi...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155973991780188160</th>\n",
       "      <td>justrp samp server bow live join websit gta</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155293840767590400</th>\n",
       "      <td>play bit swtor tri finish alderon</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154942357719105536</th>\n",
       "      <td>rt techhub pleas announc websit work much bett...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154935904992104449</th>\n",
       "      <td>gta iii goe sale holiday bring violen news tec...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154934833485856768</th>\n",
       "      <td>aol instant messeng preview version rife priv ...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154261110948769793</th>\n",
       "      <td>check thi articl techhub via techhub</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154167297697783808</th>\n",
       "      <td>dobbi garden center pancak life good mecottrel</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151990588584108032</th>\n",
       "      <td>in cafenero coffe</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151346202842107904</th>\n",
       "      <td>just got new iphon thank santa</td>\n",
       "      <td>LibDem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148466741679370240</th>\n",
       "      <td>check thi articl techhub via techhub</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146292675115630592</th>\n",
       "      <td>techhub best news around</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146291402773499904</th>\n",
       "      <td>soperifi derp</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146290898538463232</th>\n",
       "      <td>check thi articl techhub via techhub</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146288073783775232</th>\n",
       "      <td>youtub launch school servic via techhub</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146278632606810114</th>\n",
       "      <td>rt techhub yifan lu jailbreak kindl touch use ...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145951620360962048</th>\n",
       "      <td>rt techhub ipad drop lava name market news tec...</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7806442463</th>\n",
       "      <td>shotgunfre ftw shot fastest fire fastest five ...</td>\n",
       "      <td>Conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3184693760</th>\n",
       "      <td>got twitter learn</td>\n",
       "      <td>Non-Aligned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             CleanText  \\\n",
       "id                                                                       \n",
       "1102210525885288451  tim cook appl publicli commit work nvidia driv...   \n",
       "1100082605477822464  use aframevr amp cesiumj univers honour projec...   \n",
       "1008493804297613312        familysimpson notabilityapp explainevrythng   \n",
       "1008492838538096643  familysimpson notabilityapp explainevrythng tr...   \n",
       "963696549674012672   rt shopap new zerok thermal shirt avail amazon...   \n",
       "906166602013573120                                                       \n",
       "888146897235582976   secur import plexapp wed like support two fact...   \n",
       "828913387379781632              blackwakegam tjrlz edmcklabour sasrich   \n",
       "781422598458662912   forzamotorsport game pc onli run min befor cra...   \n",
       "779409641746882560   profbriancox briancoxl loreal head shoulder ki...   \n",
       "751136630572998656             rcommentari get penalti get titan theme   \n",
       "722300591855308800   rt boxcouk win netgear nighthawk modem router ...   \n",
       "634041443867299840                          nerdcubedgam hey dan scoop   \n",
       "632827417023893504   download bear brows like anoth countri thetunn...   \n",
       "629554315288576000                            your secur path internet   \n",
       "598157505416540161   threeuksupport im tenerif feel home destin wan...   \n",
       "558739132282044416   rt hannahinabucket fab lunch catch jennywardx ...   \n",
       "493851355178860545   hannahinabucket robbiethough zoeejadeexo caatw...   \n",
       "492655785043038208                just report jam n use waze social gp   \n",
       "492647635002871808             just report hazard n use waze social gp   \n",
       "489132874608967680   just report jam chapel ln blockley use waze so...   \n",
       "476312716895084544   rt rockstargam gtav come ps xbox one amp pc th...   \n",
       "462214159396913152   if ani ani spare time could fill servey colleg...   \n",
       "459266540085706752     rt michellefraas part alway want go back school   \n",
       "459265882112688130                                robbiethough xfxfxdx   \n",
       "459265657490931713      hannahinabucket onli hand satisfi lamaswithhat   \n",
       "456728504273346560                       rt jalopnik thi dodg challeng   \n",
       "456447257450000384                tjrlz mrpaul paul thi touchi subject   \n",
       "456435955562729473                                       tjrlz disagre   \n",
       "453645386909904896   rt jensonbutton wa look like strong weekend ba...   \n",
       "...                                                                ...   \n",
       "164088297604382721                            thomasbrown oh realli oo   \n",
       "164030738533056513                  it wet miser cold worst revis exam   \n",
       "162149913717784576                                    duncanjmacf inde   \n",
       "161575214101168128                                              ey see   \n",
       "161574225264652289   missamandab youtub nice video scene actual lea...   \n",
       "161449169662394368                              thomasbrown nice coupl   \n",
       "160664768099790848                                        good morn oo   \n",
       "160121368782970880                        thomasbrown kirza haha geniu   \n",
       "158976999761788929                                                       \n",
       "157763679486619649                              techhub dont work hard   \n",
       "157763574075363328   rt techhub use googl chrome youll happi know h...   \n",
       "157181188153421824   i nomin prosynd shorti award game becaus boobi...   \n",
       "155973991780188160         justrp samp server bow live join websit gta   \n",
       "155293840767590400                   play bit swtor tri finish alderon   \n",
       "154942357719105536   rt techhub pleas announc websit work much bett...   \n",
       "154935904992104449   gta iii goe sale holiday bring violen news tec...   \n",
       "154934833485856768   aol instant messeng preview version rife priv ...   \n",
       "154261110948769793                check thi articl techhub via techhub   \n",
       "154167297697783808      dobbi garden center pancak life good mecottrel   \n",
       "151990588584108032                                   in cafenero coffe   \n",
       "151346202842107904                      just got new iphon thank santa   \n",
       "148466741679370240                check thi articl techhub via techhub   \n",
       "146292675115630592                            techhub best news around   \n",
       "146291402773499904                                       soperifi derp   \n",
       "146290898538463232                check thi articl techhub via techhub   \n",
       "146288073783775232             youtub launch school servic via techhub   \n",
       "146278632606810114   rt techhub yifan lu jailbreak kindl touch use ...   \n",
       "145951620360962048   rt techhub ipad drop lava name market news tec...   \n",
       "7806442463           shotgunfre ftw shot fastest fire fastest five ...   \n",
       "3184693760                                           got twitter learn   \n",
       "\n",
       "                    Classification  \n",
       "id                                  \n",
       "1102210525885288451    Non-Aligned  \n",
       "1100082605477822464    Non-Aligned  \n",
       "1008493804297613312    Non-Aligned  \n",
       "1008492838538096643    Non-Aligned  \n",
       "963696549674012672     Non-Aligned  \n",
       "906166602013573120     Non-Aligned  \n",
       "888146897235582976     Non-Aligned  \n",
       "828913387379781632     Non-Aligned  \n",
       "781422598458662912     Non-Aligned  \n",
       "779409641746882560          Labour  \n",
       "751136630572998656     Non-Aligned  \n",
       "722300591855308800    Conservative  \n",
       "634041443867299840     Non-Aligned  \n",
       "632827417023893504          LibDem  \n",
       "629554315288576000     Non-Aligned  \n",
       "598157505416540161     Non-Aligned  \n",
       "558739132282044416             SNP  \n",
       "493851355178860545     Non-Aligned  \n",
       "492655785043038208          Labour  \n",
       "492647635002871808           Green  \n",
       "489132874608967680          Labour  \n",
       "476312716895084544     Non-Aligned  \n",
       "462214159396913152             SNP  \n",
       "459266540085706752     Non-Aligned  \n",
       "459265882112688130     Non-Aligned  \n",
       "459265657490931713     Non-Aligned  \n",
       "456728504273346560          Labour  \n",
       "456447257450000384          Labour  \n",
       "456435955562729473    Conservative  \n",
       "453645386909904896          Labour  \n",
       "...                            ...  \n",
       "164088297604382721     Non-Aligned  \n",
       "164030738533056513           Green  \n",
       "162149913717784576            UKIP  \n",
       "161575214101168128     Non-Aligned  \n",
       "161574225264652289           Green  \n",
       "161449169662394368     Non-Aligned  \n",
       "160664768099790848     Non-Aligned  \n",
       "160121368782970880     Non-Aligned  \n",
       "158976999761788929     Non-Aligned  \n",
       "157763679486619649     Non-Aligned  \n",
       "157763574075363328     Non-Aligned  \n",
       "157181188153421824     Non-Aligned  \n",
       "155973991780188160     Non-Aligned  \n",
       "155293840767590400     Non-Aligned  \n",
       "154942357719105536    Conservative  \n",
       "154935904992104449    Conservative  \n",
       "154934833485856768     Non-Aligned  \n",
       "154261110948769793     Non-Aligned  \n",
       "154167297697783808     Non-Aligned  \n",
       "151990588584108032     Non-Aligned  \n",
       "151346202842107904          LibDem  \n",
       "148466741679370240     Non-Aligned  \n",
       "146292675115630592     Non-Aligned  \n",
       "146291402773499904     Non-Aligned  \n",
       "146290898538463232     Non-Aligned  \n",
       "146288073783775232    Conservative  \n",
       "146278632606810114     Non-Aligned  \n",
       "145951620360962048     Non-Aligned  \n",
       "7806442463            Conservative  \n",
       "3184693760             Non-Aligned  \n",
       "\n",
       "[610 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def Tweetclassifer(tweet, ):\n",
    "    sentiment = sid.polarity_scores(tweet['CleanText'])\n",
    "    tweet['Sentiment'] = sentiment['compound']\n",
    "    return tweet\n",
    "UserDatad = UserData.apply(Tweetclassifer,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserDatad\n",
    "os.chdir(\"E:\\Honours\\Master\\Model\")\n",
    "UserDatad.to_csv(\"test.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
